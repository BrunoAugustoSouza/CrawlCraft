# Reuters Scraping - Automated News Scraper

---

## 📝 Project Description

This project is a Scrapy-based web scraper for collecting news articles from Reuters using Python.
It is containerized with Docker for easy deployment, and the scraper outputs results as JSON (unstructured data) or can be adapted to CSV/SQL for ETL workflows.

The scraper collects key information such as:

* Article title
* Time scraped
* URL link
* Article content
* Article section

The project is modularized as a Scrapy project for scalability and maintainability and includes advanced features like:

* Automatic proxy rotation

* Captcha bypass

* Browser impersonation
---

## 📂 Project Structure

```
reuters_scraper/
│
├── reuters_scraper/       # Scrapy project source code
│   ├── spiders/           # Spider definitions
│   ├── middlewares.py     # Custom middlewares
│   ├── pipelines.py       # Item pipelines (ETL-ready)
│   └── settings.py        # Project settings
├── scrapy.cfg             # Scrapy configuration
├── requirements.txt       # Python dependencies
└── README.md              # Project documentation

```

---

## 🚀 How to Run

### 1️⃣ Run Locally

1. Create a virtual environment (optional but recommended):

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

2. Install dependencies:

```bash
pip install -r requirements.txt
```

3. Run the Scrapy spider:

```bash
python src/main.py
```

4. Access the app in your browser:

```
scrapy crawl reuters_news -o results.json
```
Replace results.json with results.csv if you prefer CSV format.

## 📊 Example Output

You can view or download a sample output generated by the scraper here:
[📥 Example JSON Output](results.json)

---

## 🛠️ Tech Stack

Python | Scrapy | Requests

---

## 🔗 Contact / More Info

* GitHub: [Bruno Augusto](https://github.com/yourusername)
* LinkedIn: [brunoaugustosouza](https://www.linkedin.com/in/brunoaugustosouza/)

---